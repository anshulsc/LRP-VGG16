{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d4cbb3-e810-4c5c-9a8b-41dd695c14dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b13b0f-f27d-4352-85ba-d6dd8a85e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5fa31a-31d5-4ca6-bdf4-d3d11412aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd84b73-d790-4c63-b6ed-171fcbd7b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROOT = \"data/brain_mri/training\"\n",
    "TEST_ROOT = \"data/brain_mri/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39fbe673-fc91-4e5c-863e-e8e387a2e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        num_ftrs = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Linear(num_ftrs, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2b415f-c9bd-42fe-9b94-04b21448de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b21b7b1-a380-412a-9eb6-a18550ca3caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ee38a7-cee9-4730-ade0-1d68a03afce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TRAIN_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TEST_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12af1287-83e5-41c0-8ce1-b4c8e210d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b5c37bc-3677-4572-b7fc-34e728011c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b37f0448-b221-4521-8444-b9ad52a91532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is loss--> tensor(1.3697, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3748, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3691, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3429, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3807, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3712, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3602, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3816, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3607, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3494, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3546, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3758, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3366, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3510, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3635, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3573, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3254, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3156, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3235, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3485, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3114, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3327, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3157, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3301, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3100, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3202, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2971, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2909, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3252, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2936, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2941, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3131, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2603, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2966, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.3056, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2933, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2597, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2829, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2609, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2501, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2376, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2477, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2390, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2510, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2659, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2526, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2220, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1696, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2369, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1990, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2113, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2372, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2239, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2066, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1927, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1881, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2568, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1843, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1866, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2247, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1832, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2021, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.2127, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1840, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1632, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1792, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1619, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1237, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1164, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1492, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1493, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1352, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1516, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1355, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1008, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1183, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1019, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1233, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0947, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0948, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1729, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0824, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0830, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0727, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1079, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1416, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0447, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0409, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0401, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.1074, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "new epoch started\n",
      "This is loss--> tensor(1.0369, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0627, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0187, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0259, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0254, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9926, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0695, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0048, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0293, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9958, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9262, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9971, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9516, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9648, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9280, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9583, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9282, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8961, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9244, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9830, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9121, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(1.0487, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9258, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8950, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8933, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9194, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8389, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9067, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8888, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8933, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8639, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8413, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.9018, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8528, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8283, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8616, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8270, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8792, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8492, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8894, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7809, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7813, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7846, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8220, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8659, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8408, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6944, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8322, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7472, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6958, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7380, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8277, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7828, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7770, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7292, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6984, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8721, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7115, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6276, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6849, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.8283, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7414, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7626, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6900, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5745, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7551, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6380, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6296, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6903, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6964, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6705, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7041, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7145, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6900, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6498, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6373, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6059, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5999, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5571, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6950, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6394, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7230, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6399, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5478, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7083, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.7311, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5779, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5152, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5751, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5749, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "new epoch started\n",
      "This is loss--> tensor(0.5806, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4846, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4973, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5913, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5953, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6608, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4288, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5168, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5854, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5945, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5376, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4659, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4218, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6632, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4789, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4869, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5007, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5645, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5182, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5387, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3594, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4846, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5269, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5423, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4497, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5543, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4934, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4583, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4281, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4853, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4457, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4560, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4403, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5184, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5126, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4846, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4077, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5394, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4524, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4498, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5331, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4516, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3796, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3660, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3798, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3439, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4626, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5890, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4221, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3819, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4601, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4564, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3719, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4412, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4388, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4237, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4372, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5162, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3928, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5154, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4651, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2827, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5064, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4471, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3640, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3668, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4992, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5061, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.6007, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4041, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4359, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3020, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3586, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2650, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4505, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5205, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4146, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3806, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4127, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4357, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4790, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2435, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3137, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5755, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4051, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5027, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3505, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3182, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3384, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4921, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "new epoch started\n",
      "This is loss--> tensor(0.2798, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2506, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3885, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2906, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3953, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2690, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5326, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3388, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4957, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3099, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3084, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3384, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3328, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4027, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.5565, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4312, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3682, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2430, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3280, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2592, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4647, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4530, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3729, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4359, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3411, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3404, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3539, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2031, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3263, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3604, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3031, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1846, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3681, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1870, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3064, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3107, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2679, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2198, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2793, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1997, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3016, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2481, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3332, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3827, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3519, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3618, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2308, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3177, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3116, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2649, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3972, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4789, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2341, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2258, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3149, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2208, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2603, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2344, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2747, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1921, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2429, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.4085, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3259, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2504, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3206, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2362, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2143, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2898, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2638, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2772, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2063, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1898, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1649, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2823, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2505, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2374, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1773, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2039, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2777, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2268, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2578, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2292, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1736, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1913, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3379, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2288, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2769, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2879, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1730, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2020, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "new epoch started\n",
      "This is loss--> tensor(0.2134, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3271, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1361, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1238, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2344, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2439, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2501, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1427, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3233, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3323, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1623, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2828, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2819, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2016, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1178, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2998, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3739, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1607, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1703, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2400, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1757, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2599, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1408, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1682, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1902, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3190, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1192, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1565, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2996, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1711, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2140, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1055, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2029, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1955, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2557, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1454, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.0847, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1918, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1336, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1115, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1879, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3328, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1865, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1096, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2195, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2256, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1731, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2433, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1118, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1504, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1860, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2821, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2161, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.0606, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2553, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1973, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1442, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1847, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1612, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2071, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1305, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1045, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2637, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1411, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3634, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1125, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1239, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1833, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2079, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1291, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2035, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1944, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1885, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2332, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2812, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2886, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2690, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1322, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1968, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1573, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.0830, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2288, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1220, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2937, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3070, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2207, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.3783, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.2396, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1496, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "This is loss--> tensor(0.1475, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "new epoch started\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  \n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # Labels are automatically one-hot-encoded\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"This is loss-->\",loss)\n",
    "    print(\"new epoch started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc34a9-0b2e-466a-b72d-e1a8a3a2cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.numpy()\n",
    "outputs = model(inputs).max(1).indices.detach().cpu().numpy()\n",
    "comparison = pd.DataFrame()\n",
    "print(\"Batch accuracy: \", (labels==outputs).sum()/len(labels))\n",
    "comparison[\"labels\"] = labels\n",
    "\n",
    "comparison[\"outputs\"] = outputs\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66715d5b-b47f-468e-a298-477da5d8b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "nmodel = nmodel.to(device)\n",
    "\n",
    "# Modify the last fully connected layer to fit the number of classes in the brain MRI dataset\n",
    "num_classes = 4 # assuming 2 classes: tumor and non-tumor\n",
    "in_features = nmodel.fc.in_features\n",
    "nmodel.fc = nn.Linear(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5e6690-9842-4615-b290-07f4be2a62f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180ab680-4d70-4de4-9934-84c0efb3c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nmodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f53e58a-c6b3-40e1-b715-9514c1a64696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the running losss is ----> tensor(1.3809, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(1.1054, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(1.1277, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.8072, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6591, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5199, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(1.0155, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6345, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.9736, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6754, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5082, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6273, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4162, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4907, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4188, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3480, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6443, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2998, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6848, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(1.0171, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5184, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.8101, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2662, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6764, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5306, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4413, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.8373, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.8020, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5852, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2955, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.7602, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4738, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3825, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3785, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2605, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4362, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2662, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2444, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4179, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2811, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.8236, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.7260, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.7384, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3993, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4826, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4744, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5806, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5753, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2595, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1948, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4551, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4775, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6009, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3615, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3158, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4349, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4517, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2589, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4085, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5982, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3629, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4133, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1725, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3925, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3762, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2849, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5017, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5234, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2056, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2670, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2075, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6062, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2800, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3343, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3034, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3058, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2077, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4259, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1949, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1868, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2833, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2231, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1298, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2654, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3115, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2616, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3983, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1676, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2431, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3251, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "epoch loss----> 0.4760503640365933\n",
      "the running losss is ----> tensor(0.2637, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5925, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3474, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3234, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1099, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1128, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1033, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1915, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3407, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3251, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3370, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2331, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5012, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2400, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2390, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1281, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3764, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3540, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3269, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3623, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1423, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0908, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0899, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1866, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3953, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2514, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0246, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2260, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2355, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0894, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2010, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3363, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2526, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0619, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1202, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0703, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1072, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1750, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1013, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1413, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0677, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3339, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3053, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2201, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0636, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1825, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1258, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2812, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1446, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1847, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1363, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2149, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3635, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4417, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2027, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1270, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1026, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0786, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3926, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0368, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2897, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2393, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1077, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1185, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1856, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0721, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2916, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1757, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0521, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2813, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1131, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0527, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1168, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3147, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3254, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1800, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0698, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0812, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1820, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0252, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3230, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2265, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3357, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2009, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1123, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2636, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1519, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1328, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0906, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0455, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "epoch loss----> 0.20356019682377474\n",
      "the running losss is ----> tensor(0.0386, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0596, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1175, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0714, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2174, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0189, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2103, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0973, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1884, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1191, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2601, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1393, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6621, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0994, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0925, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0783, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0774, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1314, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0420, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0992, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1199, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0487, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2372, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0114, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0362, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0610, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2255, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1566, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0434, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0744, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0356, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0319, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0589, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0855, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0297, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0102, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0805, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3717, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1648, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0156, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0940, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0801, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2877, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1921, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0988, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0844, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1366, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0926, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0983, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2263, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0220, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0484, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0859, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0443, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2139, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1095, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0900, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2335, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3164, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0507, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0207, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1379, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1433, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0596, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0811, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0636, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2021, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1090, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0389, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0574, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0982, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2674, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0589, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0197, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0350, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0599, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0309, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0449, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.4032, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0486, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1523, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0796, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2149, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2071, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1606, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0307, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6978, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0784, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1275, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1002, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "epoch loss----> 0.12401771888604148\n",
      "the running losss is ----> tensor(0.1396, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1360, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1426, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1044, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1761, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2675, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0530, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1928, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1265, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2013, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0765, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0617, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0989, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0516, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0600, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0972, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1058, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0426, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0657, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0794, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1330, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0103, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0440, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3197, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0287, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1155, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1116, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1919, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1041, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1923, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1324, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1953, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2010, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0153, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0778, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0200, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0125, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0448, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1332, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2504, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1451, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0859, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0203, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1923, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2283, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0280, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0420, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0172, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0367, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0655, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1346, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0457, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1355, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0783, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1338, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0860, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0710, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3287, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0363, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0444, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1064, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1633, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1268, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0308, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0467, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0508, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0262, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0465, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2098, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.6819, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0158, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1524, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0254, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0957, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1489, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0670, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0205, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3602, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2003, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0288, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2267, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1077, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0817, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2289, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3423, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3781, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1369, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1322, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1011, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1667, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "epoch loss----> 0.12290794307346543\n",
      "the running losss is ----> tensor(0.1612, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1200, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1154, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0413, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1346, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0542, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0478, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0764, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2151, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0130, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0998, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0721, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0473, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1095, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0219, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0842, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0516, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0509, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0330, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1546, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1144, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2062, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0267, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0110, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0204, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0811, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0744, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3228, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2322, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0813, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0396, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0238, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0267, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1117, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0490, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2071, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0661, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1901, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0976, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3314, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0543, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0424, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0311, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1113, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.5849, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0856, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0258, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0299, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1408, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0556, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1864, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.3008, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1217, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0596, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0728, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1136, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0682, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0525, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0419, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0752, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0811, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0851, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1433, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0456, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0193, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0225, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1303, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1062, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0798, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0961, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0192, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0366, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0455, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0778, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0701, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0394, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0282, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0924, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0885, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2179, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0279, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0972, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0399, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0698, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1379, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0773, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0189, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.2617, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.1625, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "the running losss is ----> tensor(0.0316, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "epoch loss----> 0.09713024643188155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    running_loss = 0.0\n",
    "    vgg16.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = nmodel(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        print(\"the running losss is ---->\", loss)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(\"epoch loss---->\",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c151ed2c-d256-4a24-92c1-592b11e3b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy:  0.84375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>outputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    labels  outputs\n",
       "0        1        1\n",
       "1        3        3\n",
       "2        0        0\n",
       "3        2        2\n",
       "4        3        3\n",
       "5        1        1\n",
       "6        2        2\n",
       "7        0        0\n",
       "8        1        1\n",
       "9        2        2\n",
       "10       3        3\n",
       "11       0        1\n",
       "12       2        2\n",
       "13       0        1\n",
       "14       2        1\n",
       "15       3        3\n",
       "16       1        1\n",
       "17       0        0\n",
       "18       1        1\n",
       "19       1        1\n",
       "20       2        2\n",
       "21       3        3\n",
       "22       1        1\n",
       "23       2        2\n",
       "24       0        1\n",
       "25       0        1\n",
       "26       0        0\n",
       "27       0        0\n",
       "28       3        3\n",
       "29       1        1\n",
       "30       1        1\n",
       "31       3        3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.numpy()\n",
    "outputs = nmodel(inputs).max(1).indices.detach().cpu().numpy()\n",
    "comparison = pd.DataFrame()\n",
    "print(\"Batch accuracy: \", (labels==outputs).sum()/len(labels))\n",
    "comparison[\"labels\"] = labels\n",
    "\n",
    "comparison[\"outputs\"] = outputs\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "21c77f84-fd32-434f-a70a-b985afdb4bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from captum) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from captum) (2.1.0.dev20230412)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from captum) (3.7.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from torch>=1.6->captum) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from torch>=1.6->captum) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from torch>=1.6->captum) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from torch>=1.6->captum) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from torch>=1.6->captum) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (9.4.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (5.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->captum) (1.0.7)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7b677606-4b38-480d-9db6-e30a28c010d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 255, 255])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "image = images[30]\n",
    "label = labels[30]\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "label\n",
    "image = image.unsqueeze(0)\n",
    "image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f8ae2dae-706a-42b6-bf58-5b8ab21c298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in nmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "46759e61-89f7-40c3-87ca-55a19e8e8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output):\n",
    "    module.relevance = output.detach()\n",
    "\n",
    "for name, module in nmodel.named_modules():\n",
    "    module.register_forward_hook(forward_hook)\n",
    "    \n",
    "def lrp(model, input, output):\n",
    "    relevance = output\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            w = module.weight\n",
    "            b = module.bias\n",
    "            z = module.relevance\n",
    "            a = module.output\n",
    "            s = relevance / (a + 1e-9)\n",
    "            c = (s @ w) + b\n",
    "            relevance = z * c\n",
    "        elif isinstance(module, torch.nn.Conv2d):\n",
    "            w = module.weight\n",
    "            b = module.bias\n",
    "            z = module.relevance\n",
    "            a = module.input[0]\n",
    "            s = relevance / (a + 1e-9)\n",
    "            c = torch.nn.functional.conv2d(z, w, stride=module.stride, padding=module.padding)\n",
    "            c = c + b.view(1, -1, 1, 1)\n",
    "            relevance = z * c\n",
    "        elif isinstance(module, torch.nn.ReLU):\n",
    "            z = module.input[0]\n",
    "            a = module.output\n",
    "            s = relevance / (a + 1e-9)\n",
    "            relevance = z * (s * (a > 0).float())\n",
    "        elif isinstance(module, torch.nn.MaxPool2d):\n",
    "            z = module.input[0]\n",
    "            a = module.output\n",
    "            s = relevance / (a + 1e-9)\n",
    "            relevance = z * torch.nn.functional.max_pool2d(s, kernel_size=module.kernel_size, stride=module.stride, padding=module.padding)\n",
    "    return relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8a0ef75f-f460-4a13-99d2-3772b0605b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8bb394e6-ae6f-40d4-874d-c9723167713c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output \u001b[38;5;241m=\u001b[39m nmodel(image)\n\u001b[0;32m----> 3\u001b[0m relevance \u001b[38;5;241m=\u001b[39m \u001b[43mlrp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m relevance\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(transforms\u001b[38;5;241m.\u001b[39mToPILImage()(image\u001b[38;5;241m.\u001b[39msqueeze()), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[193], line 22\u001b[0m, in \u001b[0;36mlrp\u001b[0;34m(model, input, output)\u001b[0m\n\u001b[1;32m     20\u001b[0m b \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     21\u001b[0m z \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mrelevance\n\u001b[0;32m---> 22\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m s \u001b[38;5;241m=\u001b[39m relevance \u001b[38;5;241m/\u001b[39m (a \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m     24\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv2d(z, w, stride\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mstride, padding\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mpadding)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'input'"
     ]
    }
   ],
   "source": [
    "output = nmodel(image)\n",
    "\n",
    "relevance = lrp(nmodel, image, output)\n",
    "\n",
    "heatmap = relevance.sum(dim=1, keepdim=True).squeeze()\n",
    "\n",
    "plt.imshow(transforms.ToPILImage()(image.squeeze()), cmap='gray')\n",
    "plt.imshow(heatmap.detach().numpy(), cmap='jet', alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2ad93a31-896a-4ba9-8ef6-6ba6e5e0c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: install in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: innvestigate in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (1.0.9)\n",
      "Requirement already satisfied: future in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from innvestigate) (0.18.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from innvestigate) (1.23.5)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from innvestigate) (3.8.0)\n",
      "Collecting keras==2.2.4\n",
      "  Using cached Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "Requirement already satisfied: pytest in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from innvestigate) (7.3.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from innvestigate) (9.4.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from innvestigate) (1.10.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from keras==2.2.4->innvestigate) (1.16.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from keras==2.2.4->innvestigate) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from keras==2.2.4->innvestigate) (6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from keras==2.2.4->innvestigate) (1.1.2)\n",
      "Requirement already satisfied: iniconfig in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from pytest->innvestigate) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from pytest->innvestigate) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from pytest->innvestigate) (1.1.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from pytest->innvestigate) (23.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (from pytest->innvestigate) (2.0.1)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install install innvestigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "df2ef8f5-8ed6-494f-897f-7d822bc76530",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function forward_hook at 0x4547e2a60>: it's not the same object as __main__.forward_hook",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpretrained_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/serialization.py:443\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 443\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/serialization.py:655\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    653\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    654\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 655\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    657\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function forward_hook at 0x4547e2a60>: it's not the same object as __main__.forward_hook"
     ]
    }
   ],
   "source": [
    "torch.save(nmodel, 'pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "505b2b1a-cf07-4d26-80ed-41ce8afe1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c93e6c0e-7f6d-4428-81ec-da8aabf9fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/envs/torch/lib/python3.9/site-packages (2.2.4)\n",
      "Collecting keras\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "innvestigate 1.0.9 requires keras==2.2.4, but you have keras 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1738a71-3188-4e36-9cd2-1ef9c5c7d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a367d-6e83-4b91-9528-985e7f758bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe815-3a62-4d96-8a32-a70de355241e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64aa8c7-53c1-4262-bb3f-7c9ed9fc4d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da103b75-5cad-438f-96b8-7236830b83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d8a4f-ae6e-4eb1-b27c-276c1e7bf1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
