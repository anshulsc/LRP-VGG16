{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66485a57-abc1-4563-b64d-a5c09a532002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d424de0-b3dd-4efb-8468-5fd2c2013a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f46b6ba-00f8-4e01-9a81-0ca5027b2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2d025d-c47e-415a-8eac-8391bf1b8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROOT = \"data/brain_mri/training\"\n",
    "TEST_ROOT = \"data/brain_mri/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2d7092-2724-417f-8580-a5d1541d771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TRAIN_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((254,254)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e3e7c6-d0c0-4983-99df-9e1a57d3947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TEST_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2c90b5-52a5-4b25-8207-2205ec891e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9391130a-4028-418e-afac-99d2199e5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT) #This pretrained model is used as a feature extractor in the CNNModel class\n",
    "\n",
    "        # Replace output layer according to our problem\n",
    "        in_feats = self.vgg16.classifier[6].in_features \n",
    "        self.vgg16.classifier[6] = nn.Linear(in_feats, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg16(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd3315c-716a-4b9d-b61c-489225c0d710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (vgg16): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5318e28-0ce1-435c-b807-a4287a255ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d720c-16a8-4b23-8605-1ed06313edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is loss--> 1.364427089691162\n",
      "This is loss--> 1.3457142114639282\n",
      "This is loss--> 1.2856141328811646\n",
      "This is loss--> 1.2005751132965088\n",
      "This is loss--> 1.2985808849334717\n",
      "This is loss--> 1.2408205270767212\n",
      "This is loss--> 1.2384377717971802\n",
      "This is loss--> 1.2240166664123535\n",
      "This is loss--> 1.3347561359405518\n",
      "This is loss--> 1.2906007766723633\n",
      "This is loss--> 1.2683823108673096\n",
      "This is loss--> 1.3019572496414185\n",
      "This is loss--> 1.2208795547485352\n",
      "This is loss--> 1.3119549751281738\n",
      "This is loss--> 1.12383234500885\n",
      "This is loss--> 1.1289873123168945\n",
      "This is loss--> 1.1148428916931152\n",
      "This is loss--> 1.1269595623016357\n",
      "This is loss--> 1.0185115337371826\n",
      "This is loss--> 1.194063425064087\n",
      "This is loss--> 1.0025246143341064\n",
      "This is loss--> 1.0199826955795288\n",
      "This is loss--> 1.0158305168151855\n",
      "This is loss--> 0.9662585258483887\n",
      "This is loss--> 1.1046745777130127\n",
      "This is loss--> 1.0528247356414795\n",
      "This is loss--> 0.8544644117355347\n",
      "This is loss--> 1.0210453271865845\n",
      "This is loss--> 1.0350924730300903\n",
      "This is loss--> 1.0527796745300293\n",
      "This is loss--> 0.8489173650741577\n",
      "This is loss--> 1.056382656097412\n",
      "This is loss--> 0.8542366027832031\n",
      "This is loss--> 1.0088701248168945\n",
      "This is loss--> 0.8359082937240601\n",
      "This is loss--> 0.8307525515556335\n",
      "This is loss--> 0.7673647403717041\n",
      "This is loss--> 0.7940070033073425\n",
      "This is loss--> 0.8081800937652588\n",
      "This is loss--> 0.7668938040733337\n",
      "This is loss--> 0.6244668364524841\n",
      "This is loss--> 0.8374773263931274\n",
      "This is loss--> 0.7668200731277466\n",
      "This is loss--> 0.7509285807609558\n",
      "This is loss--> 0.8540814518928528\n",
      "This is loss--> 0.6887757778167725\n",
      "This is loss--> 0.7045447826385498\n",
      "This is loss--> 0.6942548751831055\n",
      "This is loss--> 0.7830631732940674\n",
      "This is loss--> 0.7375080585479736\n",
      "This is loss--> 0.5070780515670776\n",
      "This is loss--> 0.5273017883300781\n",
      "This is loss--> 0.5909673571586609\n",
      "This is loss--> 0.7714500427246094\n",
      "This is loss--> 0.44966891407966614\n",
      "This is loss--> 0.854849100112915\n",
      "This is loss--> 0.707055389881134\n",
      "This is loss--> 0.3158813714981079\n",
      "This is loss--> 0.5549664497375488\n",
      "This is loss--> 0.7093396186828613\n",
      "This is loss--> 0.5317648649215698\n",
      "This is loss--> 0.5654141306877136\n",
      "This is loss--> 0.4795345962047577\n",
      "This is loss--> 0.6219484806060791\n",
      "This is loss--> 0.47309938073158264\n",
      "This is loss--> 0.6167342662811279\n",
      "This is loss--> 0.6095016002655029\n",
      "This is loss--> 0.5653088092803955\n",
      "This is loss--> 0.38510119915008545\n",
      "This is loss--> 0.3834906220436096\n",
      "This is loss--> 0.4828621745109558\n",
      "This is loss--> 0.6147773265838623\n",
      "This is loss--> 0.4929555654525757\n",
      "This is loss--> 0.41885942220687866\n",
      "This is loss--> 0.4520062804222107\n",
      "This is loss--> 0.27996981143951416\n",
      "This is loss--> 0.6027597188949585\n",
      "This is loss--> 0.4045202136039734\n",
      "This is loss--> 0.39867091178894043\n",
      "This is loss--> 0.5467360019683838\n",
      "This is loss--> 0.5423510074615479\n",
      "This is loss--> 0.5123157501220703\n",
      "This is loss--> 0.31194767355918884\n",
      "This is loss--> 0.31322017312049866\n",
      "This is loss--> 0.5656068325042725\n",
      "This is loss--> 0.3436587452888489\n",
      "This is loss--> 0.6091116666793823\n",
      "This is loss--> 0.5429684519767761\n",
      "This is loss--> 0.537501871585846\n",
      "This is loss--> 0.40222224593162537\n",
      "new batch epoch started\n",
      "This is loss--> 0.29818302392959595\n",
      "This is loss--> 0.24000294506549835\n",
      "This is loss--> 0.42546528577804565\n",
      "This is loss--> 0.3910779058933258\n",
      "This is loss--> 0.3343312740325928\n",
      "This is loss--> 0.5531752705574036\n",
      "This is loss--> 0.5007283687591553\n",
      "This is loss--> 0.4107380509376526\n",
      "This is loss--> 0.425464391708374\n",
      "This is loss--> 0.2315349280834198\n",
      "This is loss--> 0.20539209246635437\n",
      "This is loss--> 0.4208684265613556\n",
      "This is loss--> 0.35692912340164185\n",
      "This is loss--> 0.5298674702644348\n",
      "This is loss--> 0.30419251322746277\n",
      "This is loss--> 0.3832840323448181\n",
      "This is loss--> 0.45253804326057434\n",
      "This is loss--> 0.29169443249702454\n",
      "This is loss--> 0.2225041687488556\n",
      "This is loss--> 0.42751210927963257\n",
      "This is loss--> 0.1874564290046692\n",
      "This is loss--> 0.35907137393951416\n",
      "This is loss--> 0.4435562789440155\n",
      "This is loss--> 0.2878683805465698\n",
      "This is loss--> 0.2941826581954956\n",
      "This is loss--> 0.324154794216156\n",
      "This is loss--> 0.34635573625564575\n",
      "This is loss--> 0.23328691720962524\n",
      "This is loss--> 0.22033114731311798\n",
      "This is loss--> 0.34671592712402344\n",
      "This is loss--> 0.5122332572937012\n",
      "This is loss--> 0.29614758491516113\n",
      "This is loss--> 0.2854834794998169\n",
      "This is loss--> 0.39460548758506775\n",
      "This is loss--> 0.433750718832016\n",
      "This is loss--> 0.28668296337127686\n",
      "This is loss--> 0.29377928376197815\n",
      "This is loss--> 0.2955951392650604\n",
      "This is loss--> 0.22344182431697845\n",
      "This is loss--> 0.28406211733818054\n",
      "This is loss--> 0.43716609477996826\n",
      "This is loss--> 0.16111497581005096\n",
      "This is loss--> 0.28458869457244873\n",
      "This is loss--> 0.14388780295848846\n",
      "This is loss--> 0.28154146671295166\n",
      "This is loss--> 0.40314871072769165\n",
      "This is loss--> 0.28636252880096436\n",
      "This is loss--> 0.43176549673080444\n",
      "This is loss--> 0.38245445489883423\n",
      "This is loss--> 0.4262852966785431\n",
      "This is loss--> 0.14688798785209656\n",
      "This is loss--> 0.32773128151893616\n",
      "This is loss--> 0.4803861379623413\n",
      "This is loss--> 0.2578963041305542\n",
      "This is loss--> 0.34937411546707153\n",
      "This is loss--> 0.46891969442367554\n",
      "This is loss--> 0.3498493432998657\n",
      "This is loss--> 0.30716753005981445\n",
      "This is loss--> 0.28661277890205383\n",
      "This is loss--> 0.42023131251335144\n",
      "This is loss--> 0.3813984990119934\n",
      "This is loss--> 0.31045109033584595\n",
      "This is loss--> 0.3163164258003235\n",
      "This is loss--> 0.38305556774139404\n",
      "This is loss--> 0.298986554145813\n",
      "This is loss--> 0.2610531151294708\n",
      "This is loss--> 0.5169259905815125\n",
      "This is loss--> 0.2541598379611969\n",
      "This is loss--> 0.17297981679439545\n",
      "This is loss--> 0.3657380938529968\n",
      "This is loss--> 0.3319457471370697\n",
      "This is loss--> 0.33683139085769653\n",
      "This is loss--> 0.16159525513648987\n",
      "This is loss--> 0.3502888083457947\n",
      "This is loss--> 0.23106028139591217\n",
      "This is loss--> 0.36367541551589966\n",
      "This is loss--> 0.3703537881374359\n",
      "This is loss--> 0.25045865774154663\n",
      "This is loss--> 0.34866857528686523\n",
      "This is loss--> 0.35988649725914\n",
      "This is loss--> 0.32919734716415405\n",
      "This is loss--> 0.42046642303466797\n",
      "This is loss--> 0.22287613153457642\n",
      "This is loss--> 0.2168823778629303\n",
      "This is loss--> 0.2664911150932312\n",
      "This is loss--> 0.10109961032867432\n",
      "This is loss--> 0.41165250539779663\n",
      "This is loss--> 0.2489861100912094\n",
      "This is loss--> 0.313270628452301\n",
      "This is loss--> 0.4919835031032562\n",
      "new batch epoch started\n",
      "This is loss--> 0.15009751915931702\n",
      "This is loss--> 0.5574022531509399\n",
      "This is loss--> 0.24407826364040375\n",
      "This is loss--> 0.1834297925233841\n",
      "This is loss--> 0.1535172015428543\n",
      "This is loss--> 0.09808652102947235\n",
      "This is loss--> 0.153879314661026\n",
      "This is loss--> 0.09438331425189972\n",
      "This is loss--> 0.17649570107460022\n",
      "This is loss--> 0.20997682213783264\n",
      "This is loss--> 0.1868039071559906\n",
      "This is loss--> 0.3182019889354706\n",
      "This is loss--> 0.1188400387763977\n",
      "This is loss--> 0.21570654213428497\n",
      "This is loss--> 0.16859188675880432\n",
      "This is loss--> 0.17962993681430817\n",
      "This is loss--> 0.3138576149940491\n",
      "This is loss--> 0.22338411211967468\n",
      "This is loss--> 0.2364359200000763\n",
      "This is loss--> 0.3103431463241577\n",
      "This is loss--> 0.4491378366947174\n",
      "This is loss--> 0.24933966994285583\n",
      "This is loss--> 0.18139782547950745\n",
      "This is loss--> 0.24365758895874023\n",
      "This is loss--> 0.3204177916049957\n",
      "This is loss--> 0.10970331728458405\n",
      "This is loss--> 0.12734206020832062\n",
      "This is loss--> 0.184080570936203\n",
      "This is loss--> 0.33515578508377075\n",
      "This is loss--> 0.18942081928253174\n",
      "This is loss--> 0.1872614622116089\n",
      "This is loss--> 0.12843725085258484\n",
      "This is loss--> 0.04926024004817009\n",
      "This is loss--> 0.1667172908782959\n",
      "This is loss--> 0.28916358947753906\n",
      "This is loss--> 0.20524023473262787\n",
      "This is loss--> 0.3115021586418152\n",
      "This is loss--> 0.30608052015304565\n",
      "This is loss--> 0.15043699741363525\n",
      "This is loss--> 0.11921925842761993\n",
      "This is loss--> 0.06632398068904877\n",
      "This is loss--> 0.12413685768842697\n",
      "This is loss--> 0.08553342521190643\n",
      "This is loss--> 0.2120741307735443\n",
      "This is loss--> 0.12322589010000229\n",
      "This is loss--> 0.030748797580599785\n",
      "This is loss--> 0.0884711816906929\n",
      "This is loss--> 0.20402033627033234\n",
      "This is loss--> 0.2450442761182785\n",
      "This is loss--> 0.1005961075425148\n",
      "This is loss--> 0.38460397720336914\n",
      "This is loss--> 0.29407668113708496\n",
      "This is loss--> 0.12822143733501434\n",
      "This is loss--> 0.17268764972686768\n",
      "This is loss--> 0.12434462457895279\n",
      "This is loss--> 0.11858818680047989\n",
      "This is loss--> 0.12506546080112457\n",
      "This is loss--> 0.261268675327301\n",
      "This is loss--> 0.16246597468852997\n",
      "This is loss--> 0.028725789859890938\n",
      "This is loss--> 0.2289559692144394\n",
      "This is loss--> 0.3944140672683716\n",
      "This is loss--> 0.10876356065273285\n",
      "This is loss--> 0.11339697986841202\n",
      "This is loss--> 0.16572709381580353\n",
      "This is loss--> 0.3480548858642578\n",
      "This is loss--> 0.047820501029491425\n",
      "This is loss--> 0.26671385765075684\n",
      "This is loss--> 0.08041426539421082\n",
      "This is loss--> 0.08889812976121902\n",
      "This is loss--> 0.058221615850925446\n",
      "This is loss--> 0.2580041289329529\n",
      "This is loss--> 0.2955659329891205\n",
      "This is loss--> 0.13763874769210815\n",
      "This is loss--> 0.30836984515190125\n",
      "This is loss--> 0.11384522169828415\n",
      "This is loss--> 0.11617002636194229\n",
      "This is loss--> 0.2024000883102417\n",
      "This is loss--> 0.18245019018650055\n",
      "This is loss--> 0.08862777054309845\n",
      "This is loss--> 0.14779913425445557\n",
      "This is loss--> 0.28726309537887573\n",
      "This is loss--> 0.26381611824035645\n",
      "This is loss--> 0.27735966444015503\n",
      "This is loss--> 0.10665453970432281\n",
      "This is loss--> 0.20698560774326324\n",
      "This is loss--> 0.13299888372421265\n",
      "This is loss--> 0.06802033632993698\n",
      "This is loss--> 0.19140291213989258\n",
      "This is loss--> 0.062027085572481155\n",
      "new batch epoch started\n",
      "This is loss--> 0.2389438897371292\n",
      "This is loss--> 0.09656165540218353\n",
      "This is loss--> 0.050867918878793716\n",
      "This is loss--> 0.3200882077217102\n",
      "This is loss--> 0.07747498154640198\n",
      "This is loss--> 0.1436510682106018\n",
      "This is loss--> 0.06637927144765854\n",
      "This is loss--> 0.11678166687488556\n",
      "This is loss--> 0.19965343177318573\n",
      "This is loss--> 0.10989309102296829\n",
      "This is loss--> 0.08481112122535706\n",
      "This is loss--> 0.2078595906496048\n",
      "This is loss--> 0.04293844476342201\n",
      "This is loss--> 0.11041203886270523\n",
      "This is loss--> 0.12772293388843536\n",
      "This is loss--> 0.1223711222410202\n",
      "This is loss--> 0.16364696621894836\n",
      "This is loss--> 0.06684167683124542\n",
      "This is loss--> 0.1703941375017166\n",
      "This is loss--> 0.15771475434303284\n",
      "This is loss--> 0.07023629546165466\n",
      "This is loss--> 0.06371468305587769\n",
      "This is loss--> 0.07606413960456848\n",
      "This is loss--> 0.052334919571876526\n",
      "This is loss--> 0.10791543126106262\n",
      "This is loss--> 0.12020742148160934\n",
      "This is loss--> 0.07312893867492676\n",
      "This is loss--> 0.07128287851810455\n",
      "This is loss--> 0.17366644740104675\n",
      "This is loss--> 0.1279391497373581\n",
      "This is loss--> 0.06885839998722076\n",
      "This is loss--> 0.24052348732948303\n",
      "This is loss--> 0.24649718403816223\n",
      "This is loss--> 0.06647193431854248\n",
      "This is loss--> 0.12636494636535645\n",
      "This is loss--> 0.0536934956908226\n",
      "This is loss--> 0.09436146169900894\n",
      "This is loss--> 0.03718772903084755\n",
      "This is loss--> 0.10763917863368988\n",
      "This is loss--> 0.045145392417907715\n",
      "This is loss--> 0.05623989552259445\n",
      "This is loss--> 0.23714983463287354\n",
      "This is loss--> 0.07255642861127853\n",
      "This is loss--> 0.2383277714252472\n",
      "This is loss--> 0.04906009882688522\n",
      "This is loss--> 0.028875697404146194\n",
      "This is loss--> 0.42039427161216736\n",
      "This is loss--> 0.1630125641822815\n",
      "This is loss--> 0.2631129324436188\n",
      "This is loss--> 0.027763454243540764\n",
      "This is loss--> 0.1786440759897232\n",
      "This is loss--> 0.09929535537958145\n",
      "This is loss--> 0.12398489564657211\n",
      "This is loss--> 0.20726366341114044\n",
      "This is loss--> 0.047526173293590546\n",
      "This is loss--> 0.04873346537351608\n",
      "This is loss--> 0.06306062638759613\n",
      "This is loss--> 0.24090254306793213\n",
      "This is loss--> 0.08709869533777237\n",
      "This is loss--> 0.048163171857595444\n",
      "This is loss--> 0.22838985919952393\n",
      "This is loss--> 0.07067659497261047\n",
      "This is loss--> 0.09445519745349884\n",
      "This is loss--> 0.06730163097381592\n",
      "This is loss--> 0.10667462646961212\n",
      "This is loss--> 0.06239508092403412\n",
      "This is loss--> 0.0469355545938015\n",
      "This is loss--> 0.019246025010943413\n",
      "This is loss--> 0.08489154279232025\n",
      "This is loss--> 0.10671575367450714\n",
      "This is loss--> 0.0445510670542717\n",
      "This is loss--> 0.0976974368095398\n",
      "This is loss--> 0.22552813589572906\n",
      "This is loss--> 0.22164581716060638\n",
      "This is loss--> 0.03476089984178543\n",
      "This is loss--> 0.10222350060939789\n",
      "This is loss--> 0.06669670343399048\n",
      "This is loss--> 0.11421038210391998\n",
      "This is loss--> 0.11755315214395523\n",
      "This is loss--> 0.10073230415582657\n",
      "This is loss--> 0.13118216395378113\n",
      "This is loss--> 0.049455076456069946\n",
      "This is loss--> 0.23786906898021698\n",
      "This is loss--> 0.1291295886039734\n",
      "This is loss--> 0.19543838500976562\n",
      "This is loss--> 0.09048590809106827\n",
      "This is loss--> 0.038893092423677444\n",
      "This is loss--> 0.13711115717887878\n",
      "This is loss--> 0.13491879403591156\n",
      "This is loss--> 0.0285185519605875\n",
      "new batch epoch started\n",
      "This is loss--> 0.03754090517759323\n",
      "This is loss--> 0.06012815982103348\n",
      "This is loss--> 0.0503731295466423\n",
      "This is loss--> 0.035277023911476135\n",
      "This is loss--> 0.08954913914203644\n",
      "This is loss--> 0.05138712376356125\n",
      "This is loss--> 0.016029082238674164\n",
      "This is loss--> 0.023986510932445526\n",
      "This is loss--> 0.04131554067134857\n",
      "This is loss--> 0.012759537436068058\n",
      "This is loss--> 0.03400058671832085\n",
      "This is loss--> 0.19809184968471527\n",
      "This is loss--> 0.03859708458185196\n",
      "This is loss--> 0.03677259758114815\n",
      "This is loss--> 0.036571256816387177\n",
      "This is loss--> 0.0383453294634819\n",
      "This is loss--> 0.017774539068341255\n",
      "This is loss--> 0.19910287857055664\n",
      "This is loss--> 0.0245240218937397\n",
      "This is loss--> 0.05153635889291763\n",
      "This is loss--> 0.1878572702407837\n",
      "This is loss--> 0.031778380274772644\n",
      "This is loss--> 0.061258528381586075\n",
      "This is loss--> 0.06906868517398834\n",
      "This is loss--> 0.05349424481391907\n",
      "This is loss--> 0.06831356883049011\n",
      "This is loss--> 0.06559833139181137\n",
      "This is loss--> 0.01727304980158806\n",
      "This is loss--> 0.024965906515717506\n",
      "This is loss--> 0.04431143403053284\n",
      "This is loss--> 0.01847272738814354\n",
      "This is loss--> 0.009332014247775078\n",
      "This is loss--> 0.024153046309947968\n",
      "This is loss--> 0.0635964572429657\n",
      "This is loss--> 0.023866552859544754\n",
      "This is loss--> 0.04642876982688904\n",
      "This is loss--> 0.07728273421525955\n",
      "This is loss--> 0.004915974102914333\n",
      "This is loss--> 0.034460294991731644\n",
      "This is loss--> 0.05327345058321953\n",
      "This is loss--> 0.009190192446112633\n",
      "This is loss--> 0.019691243767738342\n",
      "This is loss--> 0.107170470058918\n",
      "This is loss--> 0.04706043004989624\n",
      "This is loss--> 0.008452197536826134\n",
      "This is loss--> 0.13208818435668945\n",
      "This is loss--> 0.13339953124523163\n",
      "This is loss--> 0.052636824548244476\n",
      "This is loss--> 0.029690146446228027\n",
      "This is loss--> 0.013431348837912083\n",
      "This is loss--> 0.01574700139462948\n",
      "This is loss--> 0.06509602069854736\n",
      "This is loss--> 0.026626411825418472\n",
      "This is loss--> 0.13013975322246552\n",
      "This is loss--> 0.07686903327703476\n",
      "This is loss--> 0.014495376497507095\n",
      "This is loss--> 0.12791214883327484\n",
      "This is loss--> 0.05941753834486008\n",
      "This is loss--> 0.01418131310492754\n",
      "This is loss--> 0.07991273701190948\n",
      "This is loss--> 0.10959377884864807\n",
      "This is loss--> 0.060465265065431595\n",
      "This is loss--> 0.15502968430519104\n",
      "This is loss--> 0.0505220852792263\n",
      "This is loss--> 0.07273165136575699\n",
      "This is loss--> 0.13947899639606476\n",
      "This is loss--> 0.018388759344816208\n",
      "This is loss--> 0.005715044215321541\n",
      "This is loss--> 0.04885508865118027\n",
      "This is loss--> 0.022536184638738632\n",
      "This is loss--> 0.014161580242216587\n",
      "This is loss--> 0.04475976899266243\n",
      "This is loss--> 0.04662817716598511\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  \n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # Labels are automatically one-hot-encoded\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"This is loss-->\",loss.item())\n",
    "    print(\"new batch epoch started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cf877f-3f02-4478-8d66-0fc5a303a957",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_loader\u001b[49m))\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.numpy()\n",
    "outputs = model(inputs).max(1).indices.detach().cpu().numpy()\n",
    "comparison = pd.DataFrame()\n",
    "print(\"Batch accuracy: \", (labels==outputs).sum()/len(labels))\n",
    "comparison[\"labels\"] = labels\n",
    "\n",
    "comparison[\"outputs\"] = outputs\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13e34c-0726-4035-964a-4660f901f222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
